---
title: "HW4"
author: "Bedros, Haig; Dela Cruz, Sandra; Hugh, Tiffany; Li, Yanyi"
date: "`r Sys.Date()`"
output: 
editor_options: 
  markdown: 
    wrap: 72
---
<style type="text/css">

h1.title {
  font-size: 26px;
  color: Black;
  text-align: center;
}
h4.author { 
  font-size: 18px;
  color: Black;
  text-align: center;
}
h4.date {
  font-size: 12px;
  color: Black;
  text-align: center;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 50), tidy = TRUE, warning = FALSE)
```

```{r, include=FALSE}
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(purrr)
library(DataExplorer)
library(reshape2)
library(pROC)
library(readr)
library(caret)
library(DHARMa)
library(lares)
library(mice)
library(glmnet)
```

```{r, include=FALSE}
# Import dataset
insurance_training_data <- read_csv("https://raw.githubusercontent.com/tiffhugh/DATA-621/refs/heads/main/Assignment%20%234/insurance_training_data.csv")
insurance_evaluation_data<- read_csv("https://raw.githubusercontent.com/tiffhugh/DATA-621/refs/heads/main/Assignment%20%234/insurance-evaluation-data%20(1).csv")
```

# **Data Exploration**

First, let's remove the 'INDEX' column in both of the dataset, since we are not going to need it.
```{r, echo=FALSE}
# remove index columns
insurance_training_data <- insurance_training_data %>% select(-INDEX)
glimpse(insurance_training_data)
dim(insurance_training_data)

# do the same for evaluation data
insurance_evaluation_data <- insurance_evaluation_data %>% select(-INDEX)
```
Our insurance training dataset has 8,161 observations and 25 columns. As we can see, there are a lot of data transformations that needs to be done with our variables, like cleaning our values and changing the columns into categorical or numerical.

### Data Transformation of Variables
First, let's clean our values by removing unnecessary characters such as 'z_', '$', ',', and '-'.
```{r, echo=FALSE}
# clean the values in our variables by removing 'z_', '$', and ','
insurance_training_data <- insurance_training_data %>% 
  mutate(across(everything(), ~gsub("z_|\\$|\\,|\\-","", .x)))
glimpse(insurance_training_data)

insurance_evaluation_data <- insurance_evaluation_data %>%
  mutate(across(everything(), ~gsub("z_|\\$|\\,|\\-", "", .x)))
```
Now that we have removed these unnecessary characters in our values, let's review our distinct values.
```{r, echo=FALSE}
# check distinct values for all columns
insurance_training_data %>% summarise(across(everything(), ~n_distinct(.)))

# check unique values in possibly categorical columns
insurance_training_data %>% 
  distinct(KIDSDRIV, PARENT1, MSTATUS, SEX, EDUCATION, JOB, CAR_USE, CAR_TYPE,
           RED_CAR, REVOKED, URBANICITY) %>% 
  map(~unique(.))
```
As we can see, 'NA' is present for the `JOB` column. Further investigation with our dataset showed that these individuals with 'NA' in the `JOB` column completed either 'Masters' or 'PhD'; however, we'll fill in the 'NA' with 'Other' just for general consideration and also to prevent bias. 
```{r, echo=FALSE}
# fill the missing 'JOB' values with 'Self-employed/ Freelance'
insurance_training_data <- insurance_training_data %>% 
  mutate(JOB = ifelse(is.na(JOB), 'Other', JOB))
# view changes
insurance_training_data %>% 
  distinct(JOB)

# do the same for evaluation data
insurance_evaluation_data <- insurance_evaluation_data %>% 
  mutate(JOB = ifelse(is.na(JOB), 'Other', JOB))
```
Now, let's proceed with converting our columns to the appropriate data types, such as factors, numeric, or ordinal variables
Convert to factor variables: `TARGET_FLAG`, `KIDSDRIV`, `PARENT1`, `MSTATUS`, `SEX`, `EDUCATION`, `JOB`, `CAR_USE`, `CAR_TYPE`, `RED_CAR`, `REVOKED`, `URBANICITY`
Convert to numeric variables: `TARGET_AMT`, `AGE`, `HOMEKIDS`, `YOJ`, `INCOME`, `HOME_VAL`, `TRAVTIME`, `BLUEBOOK`, `TIF`, `OLDCLAIM`, `CAR_AGE`, `URBANICITY`
Convert to ordinal variables: `CLM_FREQ`, `MVR_PTS`
```{r, echo=FALSE}
insurance_training_data <- insurance_training_data %>% 
  mutate(
    # convert all factor variables
    across(c(TARGET_FLAG, KIDSDRIV, PARENT1, MSTATUS, SEX, EDUCATION, JOB, CAR_USE,
             CAR_TYPE, RED_CAR, REVOKED, URBANICITY), as.factor),
    # convert all numeric variables
    across(c(TARGET_AMT, AGE, HOMEKIDS, YOJ, INCOME, HOME_VAL, TRAVTIME,
             BLUEBOOK, TIF, OLDCLAIM, CAR_AGE, URBANICITY), as.numeric),
    # convert all ordinal variables
    across(c(CLM_FREQ, MVR_PTS), as.ordered)
  )

glimpse(insurance_training_data)

# do the same for evaluation data
insurance_evaluation_data <- insurance_evaluation_data %>% 
  mutate(
    # convert all factor variables
    across(c(TARGET_FLAG, KIDSDRIV, PARENT1, MSTATUS, SEX, EDUCATION, JOB, CAR_USE,
             CAR_TYPE, RED_CAR, REVOKED, URBANICITY), as.factor),
    # convert all numeric variables
    across(c(TARGET_AMT, AGE, HOMEKIDS, YOJ, INCOME, HOME_VAL, TRAVTIME,
             BLUEBOOK, TIF, OLDCLAIM, CAR_AGE, URBANICITY), as.numeric),
    # convert all ordinal variables
    across(c(CLM_FREQ, MVR_PTS), as.ordered)
  )
```
### Summary of Datasets
Here we see 'NA' values with `AGE`, `YOJ`, `INCOME`, `HOME_VAL`, and `CAR_AGE`. Later we'll impute each of these missing values.

```{r}
# Create comprehensive summary statistics table
summary_stats <- insurance_training_data %>%
  select(where(is.numeric)) %>%
  pivot_longer(everything(), names_to = "Variable") %>%
  group_by(Variable) %>%
  summarise(
    Mean = mean(value, na.rm = TRUE),
    SD = sd(value, na.rm = TRUE),
    Median = median(value, na.rm = TRUE),
    Min = min(value, na.rm = TRUE),
    Max = max(value, na.rm = TRUE),
    NAs = sum(is.na(value)),
    .groups = 'drop'
  ) %>%
  mutate(across(where(is.numeric), ~round(., 2)))

# Display as formatted table
knitr::kable(summary_stats, caption = "Summary Statistics of Numeric Variables")
```

### Histogram of Continuous Variables
```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10
plot_histogram(insurance_training_data)
plot_boxplot(insurance_training_data, by='TARGET_FLAG')
```
With the view of our histogram, we can see that most of our continuous data are skewed to the right.

### Bar Chart of Categorical Variables
```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10
plot_bar(insurance_training_data)
```
Our bar chart reveals a lot about our dataset, such as the fact that most of our customers have 'Blue Collar' jobs, own either an 'SUV' or 'Minivan', are 'High School Graduates' or hold a 'Bachelor's' degree, are single parents (based on `PARENT1`), do not have a ticket (based on `MVR_PTS`), and live in urban areas.

### Boxplot of Some Variables
```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10
plot_boxplot(insurance_training_data, by='TARGET_FLAG')
```
The boxplot above separates the data by the TARGET_FLAG variable. Although there are noticeable outliers in our data, we will retain them as they make sense in real-world scenarios. For example, someone could have worked for a long time at a particular company (variable `YOJ`), or someone might use their car for commercial purposes (variable `TRAV_TIME`).

### Visual Correlation of Variables
```{r, echo=FALSE, message=FALSE}
#| fig-height: 5
#| fig-width: 10
corr_cross(insurance_training_data, 
           method = "spearman", max_pvalue = 0.05, top = 20)

corr_cross(insurance_training_data, 
           method = "spearman", max_pvalue = 0.05, 
           contains = c('TARGET_AMT', 'TARGET_FLAG'))
```
To visualize correlations, we use the 'corr_cross' function from the 'lares' package, which allows us to easily compare variables. We set the method to 'Spearman' since we know that most of our data is not normally distributed. In the first cross-correlation graph, we can see that 'OLDCLAIM' is highly correlated with 'CLM_FREQ,' with correlations gradually decreasing as we move down the list to variables with weaker correlations. In the second correlation graph, we observe that our predictor variables are not strongly correlated with the target variables. With all this information, we can now proceed with our data preparation.

# **Data Preparation**
### Change of Variable
For the variable `HOME_VAL`, we observed that there are 2,294 instances where the value is zero. We have made the assumption that these individuals are likely renters, as they do not own a home. For the records where `HOME_VAL` is missing, we have decided to label them as 'Unknown'. This is because we are unsure of their homeownership status, and by assigning them as 'Unknown', we avoid introducing any bias into our analysis.

```{r, echo=FALSE}
# Count zero's in 'HOME_VAL'
zero_count <- sum(insurance_training_data$HOME_VAL == 0, na.rm = TRUE)
cat("Zero count for HOME_VAL: \n")
print(zero_count)
```

In this step, we are creating a new variable called `HOME_STATUS` based on the values of `HOME_VAL`:

- Individuals with`HOME_VAL` > 0 will be categorized as 'Homeowners'.

- Individuals with `HOME_VAL` == 0 will be categorized as 'Renters'.

- Missing values (NA) in `HOME_VAL` will be labeled as 'Unknown', as we cannot determine their homeownership status.
```{r, echo=FALSE}
# create new variable 'HOME_STATUS' to separate renters, homeowners, and unknown
insurance_training_data <- insurance_training_data %>% 
  mutate(HOME_STATUS = case_when(
    is.na(HOME_VAL) ~ "Unknown",
    HOME_VAL == 0 ~"Renter",
    TRUE ~ "Homeowner"
  )) %>% 
  mutate(HOME_STATUS = factor(HOME_STATUS, levels = c("Homeowner", "Renter", "Unknown")))

head(insurance_training_data[, c("HOME_VAL", "HOME_STATUS")])

# do the same for evaluation dataset
insurance_evaluation_data <- insurance_evaluation_data %>% 
  mutate(HOME_STATUS = case_when(
    is.na(HOME_VAL) ~ "Unknown",
    HOME_VAL == 0 ~"Renter",
    TRUE ~ "Homeowner"
  )) %>% 
  mutate(HOME_STATUS = factor(HOME_STATUS, levels = c("Homeowner", "Renter", "Unknown")))
```

Now that we categorize the homeownership, let's delete the `HOME_VAL` column for us to proceed in other imputation of our columns.
```{r, echo=FALSE}
insurance_training_data <- insurance_training_data %>% 
  select(-HOME_VAL)
colnames(insurance_training_data)

insurance_evaluation_data <- insurance_evaluation_data %>% 
  select(-HOME_VAL)
```
### Imputation of Variables
For the variables `AGE`, `YOJ`, `INCOME`, and `CAR_AGE`, we will use Predictive Mean Matching (PMM) imputation from the 'mice' package. PMM is a general imputation method that ensures missing values are replaced with observed values, helping to maintain the original relationships in the data, including non-linear ones. We will set the number of imputations (m) to 5, as the dataset is large and this is a standard choice. The default maximum number of iterations (maxit) will remain at 5.

We selected the fifth iteration of AGE to fill in the missing values, as it seems reasonable that the imputed values are near the mean. Additionally, as shown below, the histogram retains its original shape even after the missing values were imputed.
```{r, echo=FALSE}
# mice imputation
input_training = insurance_training_data
method_vector_train <- c("", "", "", "pmm", "",
                   "pmm", "pmm", "", "", "",
                   "", "", "", "", "",
                   "", "", "", "", "",
                   "", "", "pmm", "", ""
                   )
mice_imp_train = mice(input_training, m = 5, 
                method = method_vector_train)
summary(input_training$AGE)
mice_imp_train$imp$AGE
clean_insurance_training_data <- complete(mice_imp_train, 4)
summary(clean_insurance_training_data)
plot_histogram(clean_insurance_training_data)

# do the same for evaluation data
input_evaluation = insurance_evaluation_data
method_vector_eval <- c("", "", "", "pmm", "",
                   "pmm", "pmm", "", "", "",
                   "", "", "", "", "",
                   "", "", "", "", "",
                   "", "", "pmm", "", ""
                   )
mice_imp_eval = mice(input_evaluation, m = 5, 
                method = method_vector_eval)
clean_insurance_evaluation_data <- complete(mice_imp_eval, 4)
```

### TARGET_FLAG Distribution

Now that the data is clean and imputed, we want to confirm the distribution of people who crashed vs. who didn't crash.

```{r}
# making sure TARGET_FLAG is numeric
insurance_training_data$TARGET_FLAG <- as.numeric(as.character(insurance_training_data$TARGET_FLAG))

# choosing relevant predictors
logit_data <- insurance_training_data %>%
  drop_na(AGE, INCOME, MSTATUS, SEX, CAR_USE, EDUCATION, RED_CAR, CLM_FREQ, MVR_PTS)

# confirming the distribution of people who crashed vs. didn't crash
table(logit_data$TARGET_FLAG)
```
The output means we've got 5672 people who didn’t crash and 2039 who did, so the classes are a bit imbalanced (about 22% had crashes). It's not too bad, but something to keep in mind later.


### Logisitic Regresion Model
Now that we have the distribution, we build a logistic regression model to predict the chance someone gets into a car crash (TARGET_FLAG). We included variables like age, income, driving history, and education, variables that might reasonably relate to risk. 

```{r}
logit_model <- glm(
  TARGET_FLAG ~ AGE + INCOME + MSTATUS + SEX + CAR_USE + EDUCATION + RED_CAR + CLM_FREQ + MVR_PTS,
  data = clean_insurance_training_data,
  family = binomial
)

summary(logit_model)
```
- AGE and INCOME are both negatively related to crash risk — makes sense, older and higher-income people might drive more cautiously.
- Being married (MSTATUSYes), using a car privately, and being male all lower the odds of a crash in this model.
- Education matters — people with a Bachelors, Masters, or PhD show lower crash risk than those with less education, though High School level wasn’t significant.
- Claim frequency (CLM_FREQ.L) has a significant positive effect — not surprising, people who’ve filed more claims are more likely to have another.
- RED_CAR isn’t significant — which kind of proves the myth of red cars being more prone to accidents.
- MVR_PTS came in with a lot of noise and no clear pattern — possibly too granular or sparse when treated as ordered.

### Logisitic Regresion Model Evaluation

Now used the model to predict crashes (TARGET_FLAG) and compared it to the actual results. 
```{r}
# get predicted probabilities
logit_preds <- predict(logit_model, type = "response")

# convert to 0/1 predictions using 0.5 threshold
logit_class <- ifelse(logit_preds > 0.5, 1, 0)

# the confusion matrix
table(Predicted = logit_class, Actual = logit_data$TARGET_FLAG)

# accuracy
mean(logit_class == logit_data$TARGET_FLAG)
```
- 5380 people correctly predicted not to crash (true negatives)
- 460 correctly predicted to crash (true positives)
- 1579 false negatives — people who did crash, but we predicted they wouldn’t
- 292 false positives — predicted to crash, but they didn’t

The model achieves 75.7% accuracy, which is solid, especially considering the class imbalance. Since crashes only occur in about 22% of cases, the model is naturally skewed toward predicting non-crashes, which isn’t necessarily a problem in this context.

Still, to explore if we can improve the model’s sensitivity to actual crashes, we’ll try undersampling the majority class and see how it impacts performance.
```{r}
library(dplyr)

# Separate majority and minority classes
majority <- logit_data %>% filter(TARGET_FLAG == 0)
minority <- logit_data %>% filter(TARGET_FLAG == 1)

# Randomly sample from the majority class
set.seed(123)
majority_sample <- majority %>% sample_n(nrow(minority))

# Combine the balanced dataset
balanced_data <- bind_rows(majority_sample, minority)

# Check the distribution of the target variable
table(balanced_data$TARGET_FLAG)
```

Now that we extracted a random balanced sample from our dataset, let's measure the accuracy:
```{r}
logit_model <- glm(
  TARGET_FLAG ~ AGE + INCOME + MSTATUS + SEX + CAR_USE + EDUCATION + RED_CAR + CLM_FREQ + MVR_PTS +
    AGE:INCOME + MSTATUS:CAR_USE,
  data = balanced_data,
  family = binomial
)

summary(logit_model)

# get predicted probabilities
logit_preds <- predict(logit_model, newdata = logit_data, type = "response")

# convert to 0/1 predictions using 0.5 threshold
logit_class <- ifelse(logit_preds > 0.5, 1, 0)

# the confusion matrix
table(Predicted = logit_class, Actual = logit_data$TARGET_FLAG)

# accuracy
mean(logit_class == logit_data$TARGET_FLAG)
```
After undersampling, the model's accuracy dropped from 75.7% to 67.5%, but it correctly identified far more actual crashes (1,333 vs. 460 true positives). If our goal is to improve crash detection — which is important in an insurance setting — the undersampled model is the better choice, especially since we're keeping the prediction threshold fixed at 0.5.

We will do ROC curve & AUC to further confirm the accuracy of the model.

### ROC curve & AUC

```{r}
library(pROC)

roc_obj <- roc(logit_data$TARGET_FLAG, logit_preds)
plot(roc_obj, main = "ROC Curve")
auc(roc_obj)
```
The AUC (Area Under the Curve) tells us how well the model separates crashers from non-crashers. 
0.73 means our model is doing a decent job. It’s better than random guessing (which would be 0.5), and it shows that the model is capturing useful patterns in the data.

### Stepwise Selected Logisitic Regresion Model

To compare the results from the manually selected logistic regression model, we will perform a stepwise selected logistic regression model.

```{r}
# Prepare the full model formula (excluding INDEX and TARGET_AMT)
full_formula <- as.formula(
  paste("TARGET_FLAG ~", 
        paste(names(clean_insurance_training_data)[!names(clean_insurance_training_data) %in% 
                                                    c("INDEX", "TARGET_AMT")], 
              collapse = " + "))
)

# Create null model (intercept only)
null_model <- glm(TARGET_FLAG ~ 1, 
                 family = binomial, 
                 data = clean_insurance_training_data)

# Perform stepwise selection (both directions)
set.seed(1225) # For reproducibility
stepwise_model <- step(null_model,
                      scope = list(lower = ~1, 
                                  upper = full_formula),
                      direction = "both",
                      trace = 1,  # Show steps in console
                      k = log(nrow(clean_insurance_training_data))) # BIC penalty

# View final model summary
summary(stepwise_model)
```



