---
title: "Crimes"
author: "Bedros, Haig; Dela Cruz, Sandra; Hugh, Tiffany; Li, Yanyi"
date: "2025-03-18"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include=FALSE}
#install.packages("correlationfunnel")
#install.packages("caret")
#install.packages("DHARMa")
library(dplyr)
library(ggplot2)
library(tidyr)
library(DataExplorer)
library(correlationfunnel)
library(reshape2)
library(pROC)
library(readr)
library(caret)
#library(DHARMa)
```

```{r, include=FALSE}
# Import dataset
crime_training_data <- read_csv("https://raw.githubusercontent.com/tiffhugh/DATA-621/refs/heads/main/Assignment%20%233/crime-training-data_modified.csv")
View(crime_training_data)

crime_evaluation_data<- read_csv("https://raw.githubusercontent.com/tiffhugh/DATA-621/refs/heads/main/Assignment%20%233/crime-evaluation-data_modified.csv")
View(crime_evaluation_data)
```

### Objective 
For our third group assignment, we were provided with a dataset containing crime-related information for various neighborhoods in a major city. The dataset includes a binary response variable indicating whether a neighborhood’s crime rate is above the median (1) or below it (0). Our goal is to develop a binary logistic regression model using the training dataset to predict whether a given neighborhood is at risk for high crime levels. Through this analysis, we aim to identify key predictors of crime and assess the model's effectiveness in classification.

The variables in this data compromise of: 
• zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)

• indus: proportion of non-retail business acres per suburb (predictor variable)

• chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)

• nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)

• rm: average number of rooms per dwelling (predictor variable)

• age: proportion of owner-occupied units built prior to 1940 (predictor variable)

• dis: weighted mean of distances to five Boston employment centers (predictor variable)

• rad: index of accessibility to radial highways (predictor variable)

• tax: full-value property-tax rate per $10,000 (predictor variable)

• ptratio: pupil-teacher ratio by town (predictor variable)

• lstat: lower status of the population (percent) (predictor variable)

• medv: median value of owner-occupied homes in $1000s (predictor variable)

• target: whether the crime rate is above the median crime rate (1) or not (0) (response variable) 


# **Data Exploration** 

```{r, echo=FALSE}
cat("Summary of training data: \n")
summary(crime_training_data)

cat("Summary of evaluation data: \n")
summary(crime_evaluation_data)
```

Preliminary summary statistics reveal that the crime training dataset consists of 466 observations with 12 predictor variables, while the crime evaluation dataset contains 40 observations with 13 variables, including the target variable. Our next step is to examine the distribution of the predictor variables to gain deeper insights into their characteristics and potential impact on the model.

### Histogram of Predictor Variables
```{r echo=FALSE}
#| fig-height: 5
#| fig-width: 10
# let's plot a histogram using DataExplorer library
plot_histogram(crime_training_data)
```

The histograms reveal distinct distribution patterns among the variables, shedding light on their underlying characteristics. The zn variable, representing the proportion of land zoned for large lots, is heavily right-skewed, indicating that most suburbs have little to no large-lot zoning, with only a few having substantial allocations. The indus variable follows a moderately right-skewed distribution, suggesting that non-retail business acreage is relatively low in most suburbs but significantly higher in a few areas. The chas variable, being binary (0 or 1), is highly imbalanced, with most suburbs not bordering the Charles River. The nox variable, which measures air pollution, has a somewhat uniform but slightly skewed distribution, reflecting variability in nitrogen oxide concentrations across different areas.

The rm variable, capturing the average number of rooms per dwelling, is approximately normal, implying that most homes have a standard number of rooms, with fewer extreme cases. The age variable is skewed toward higher values, highlighting that a significant proportion of owner-occupied units were built before 1940. The dis variable is right-skewed, meaning that while some suburbs are close to employment centers, many are farther away. The rad and tax variables exhibit a bimodal distribution, which makes sense given their categorical-like nature—certain suburbs have particularly high accessibility to highways or high property tax rates, while others do not.

The ptratio variable is also right-skewed, showing that most suburbs have lower pupil-teacher ratios, with a few having significantly higher ones. The lstat variable, representing the percentage of lower-status population, has a broader spread, signifying substantial variation in socioeconomic conditions across suburbs. Finally, the medv variable, indicating the median home value, follows a near-normal distribution but with slight skewness, suggesting that while home values are mostly centered around a typical range, some suburbs have notably high property values. These distribution characteristics highlight potential areas for transformation or deeper statistical exploration.

# **Data Preparation**

### Missing Data

```{r, echo=FALSE}
#| fig-height: 3
#| fig-width: 8
plot_missing(crime_training_data)
```

As we can see in the graph above that there is no missing data with our any
of our predictor variables.

### Outliers

```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10
plot_boxplot(crime_training_data, by='target')
```
The boxplot above separates the data by the 'target' variable, showing the distribution of crime rates for the two target groups (0 and 1). The median line inside each box indicates the central value of the crime rate for each group. From the boxplot, we can observe that the distributions of crime rates for both target = 0 (box below) and target = 1 (box above) are quite widely spread. This suggests that there is significant variability in the crime rate for both groups, with several outliers beyond the whiskers. Given the broad distribution in both groups, we choose to retain the outliers in our analysis, as they may represent important extreme events, such as rare high-crime instances. 

### Combine Variables

Older homes might be located in neighborhoods with lower home values, and this combination could provide insight into areas that may be socioeconomically distressed and more prone to higher crime rates. Therefore, we'll introduce a new variable called "housing_age_value_ratio." This variable measures the ratio of the proportion of owner-occupied units built prior to 1940 (age) divided by the median value of owner-occupied homes in thousands of dollars (medv).

```{r, echo=FALSE}
crime_training_data$housing_age_value_ratio <- 
  crime_training_data$age / crime_training_data$medv
head(crime_training_data[, c("age", "medv", "housing_age_value_ratio")])
```

### Correlation Funnel
```{r, echo=FALSE}
# Step 1: Convert to Binary Format
crime_training_data_binarized_tbl <- crime_training_data %>% 
  binarize(n_bins = 4, thresh_infreq = 0.01)

# Step 2: Perform Correlation Analysis
crime_training_data_correlated_tbl <- crime_training_data_binarized_tbl %>% 
  correlate(target = target__1)

head(crime_training_data_correlated_tbl)
```

```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10
# Step 3: Visualize the Correlation Funnel
crime_training_data_correlated_tbl %>%
   plot_correlation_funnel(interactive = FALSE)
```
By using the correlation funnel above, we can guide our decision on which variables to choose when building our model. Our target variable for this funnel is target = 1, which indicates that the crime rate is above the median crime rate. The variables at the top of the funnel have the highest correlation with the target, and as we move down the funnel, the correlation becomes weaker.

# **Build Models**

### Model #1 : General Linear Models


```{r, echo=TRUE}
# build glm
logit_model_1 <- glm(target ~ ., 
                     data = crime_training_data, 
                     family = binomial)

# summarize the model
summary(logit_model_1)
```

The first model selected a general linear model to observe the relationships between all the predictors and the target variable. GLM is straightforward and interpretable baseline for predicting whether a neighborhood has a high crime rate. By including all predictor variables initially, we can identify which factors are statistically significant before refining the model. This helps ensure we don’t prematurely exclude important variables and allows us to justify any removals based on evidence rather than assumptions.

The significant predictors include nox, dis, rad, tax, ptratio, and medv:

• Nitrogen oxides concentration: The coefficient is 49.34, the largest positive coefficient, indicating that higher levels of nitrogen oxides are strongly associated with higher crime rates. This is statistically significant with a p-value of 4.61e-10, making it a strong predictor. This result is intuitive, as areas with higher pollution levels could indicate lower socioeconomic conditions, which may correlate with higher crime rates.

• Distance to employment centers: The coefficient is 0.73, suggesting that a higher distance to employment centers increases the odds of a neighborhood having a high crime rate. With a p-value of 0.00160, this relationship is statistically significant.

• Accessibility to radial highways: The coefficient is 0.71, meaning that higher accessibility to radial highways increases the likelihood of a high crime rate. This is statistically significant with a p-value of 3.64e-05. While it's not entirely surprising that better highway access could correlate with higher crime (since highways can facilitate criminal activity), it’s still a bit counterintuitive because areas with better infrastructure might also be expected to have more resources that could reduce crime.

• Property tax rate: The coefficient is -0.0073, indicating that a higher property tax rate slightly decreases the odds of a neighborhood having a high crime rate. This is statistically significant with a p-value of 0.02249. This is counterintuitive because one might expect higher property taxes to correlate with higher crime in economically disadvantaged areas. However, it could be related to gentrification, where rising property taxes are often associated with neighborhood revitalization, improved infrastructure, and a decrease in crime.

• Pupil-teacher ratio: The coefficient is 0.39, suggesting that a higher pupil-teacher ratio increases the odds of a neighborhood having a high crime rate. With a p-value of 0.00186, this relationship is statistically significant.

• (Median value of owner-occupied homes: The coefficient is 0.21, suggesting that higher home values are associated with higher crime rates. This is statistically significant with a p-value of 0.00475.

Other predictors, such as zn, indus, chas, rm, age, lstat, and housing_age_value_ratio, are not significant predictors:

• Housing_age_value_ratio: The coefficient is 0.27, indicating a slight positive association with high crime rates, but the p-value of 0.28631 suggests it is not statistically significant.

• Dummy for bordering the Charles River: The coefficient is 0.95, indicating that neighborhoods bordering the Charles River have higher odds of having a high crime rate. However, the p-value of 0.21422 suggests this relationship is not statistically significant.

• Proportion of non-retail business acres: The coefficient is -0.06, meaning that a one-unit increase in the proportion of non-retail business acres decreases the log-odds of a high crime rate by approximately 0.063. The p-value of 0.18764 indicates that this predictor is not statistically significant and might not be useful in predicting crime rates.

This analysis suggests that while some predictors significantly influence crime rates, others do not show strong associations, and their inclusion in the model could be reconsidered. A transformation can be done to see how more align predictors are to the target. 

Now that we've run a GLM, the next step is to evaluate how well this model predicts crime. We'll start by running a residual diagnostic to assess model fit.
```{r, echo=TRUE}
library(DHARMa)
# plot residual vs. predicted plot
sr1 <- simulateResiduals(logit_model_1)
plot(sr1)
```

The residual diagnostics indicates that the binary logistic regression model fits the data well. The QQ plot shows that residuals closely follow the expected uniform distribution, with no significant deviations detected by the KS, dispersion, or outlier tests (all p-values > 0.05). Additionally, the residuals versus predicted values plot exhibits no systematic patterns, suggesting that model assumptions are adequately met. Overall, the model appears to be appropriate for predicting high crime rates based on the given predictors.

```{r, echo=FALSE}
# get the log likelihood for comparison with other models
ll_model1 <- logLik(logit_model_1)
ll_model1
```
In our crime prediction model, the log-likelihood of -95.5015 shows how well the model fits the data. While the model fits reasonably well, there's still room for improvement, as a log-likelihood closer to zero would indicate a better fit. The df=14 means the model has 14 estimated parameters (including predictors and the intercept). As we move forward, we’ll use the Confusion Matrix, ROC curve, and AUC to evaluate the model’s performance and identify areas for improvement

```{r, echo=FALSE}
### Model 1 (Confusion Matrix, ROC curve, and AUC value)
# Predict probabilities on the training data
prediction_prob_train1 <- predict(logit_model_1, crime_training_data, type="response")

# Predict classes (binary classification using a threshold of 0.5)
threshold <- 0.5
prediction_class_train1 <- ifelse(prediction_prob_train1 > threshold, 1, 0)
```
```{r, echo=TRUE}
# Calculate performance metrics on the training dataset
# Confusion Matrix
conf_matrix_train1 <- confusionMatrix(as.factor(prediction_class_train1), 
                                     as.factor(crime_training_data$target))
print(conf_matrix_train1)
```
We are using a confusion matrix to assess the model's performance. The confusion matrix shows an accuracy of 91.63%, indicating that the model correctly predicted the crime rate classification in nearly 92% of cases. The sensitivity of 92.83% demonstrates the model's strong ability to identify neighborhoods with high crime, while the specificity of 90.39% shows it can accurately identify neighborhoods with low crime. Additionally, the model’s precision is 90.91%, meaning it’s highly reliable when predicting high-crime areas, and the negative predictive value of 92.41% indicates a high accuracy for predicting low-crime areas. The p-value of 0.5218 suggests no significant difference between false positives and false negatives. While the model is performing well, there is still room for improvement, and further analysis with the ROC curve and AUC will help refine the model's performance.
```{r, echo=FALSE}
# Accuracy
accuracy1 <- sum(prediction_class_train1 == crime_training_data$target) / 
  length(crime_training_data$target)
cat("Accuracy: ", accuracy, "\n")

# Classification Error Rate
classification_error_rate1 <- 1 - accuracy1
cat("Classification Error Rate: ", classification_error_rate1, "\n")

# Precision
precision1 <- conf_matrix_train1$byClass["Pos Pred Value"]
cat("Precision: ", precision1, "\n")

# Sensitivity (Recall or True Positive Rate)
sensitivity1 <- conf_matrix_train1$byClass["Sensitivity"]
cat("Sensitivity: ", sensitivity1, "\n")

# Specificity (True Negative Rate)
specificity1 <- conf_matrix_train1$byClass["Specificity"]
cat("Specificity: ", specificity1, "\n")

# F1 Score
f1_score1 <- 2 * (precision1 * sensitivity1) / (precision1 + sensitivity1)
cat("F1 Score: ", f1_score1, "\n")
```
```{r, echo=FALSE}
# AUC (Area Under the Curve)
roc_curve1 <- roc(crime_training_data$target, prediction_prob_train1)
auc_value1 <- auc(roc_curve1)
cat("AUC: ", auc_value1, "\n")

# Plot ROC curve with AUC in the title
plot(roc_curve1, main = paste("ROC Curve Model 1 - AUC:", 
                             round(auc(roc_curve1), 2)), col = "red3")
```
The Area Under the Curve (AUC) test is important for evaluating the performance of the logistic regression model, as it provides a single value that summarizes the model's ability to distinguish between the two classes—high and low crime neighborhoods. A higher AUC indicates a better-performing model. The results show an AUC of 0.9744, which indicates that the model has excellent discriminative ability, performing well in distinguishing between neighborhoods with high and low crime rates.

# **Predictions Using Evaluation Data Set**
Now that we know the model is valid, we can apply the model to the evaluation data. 
```{r}
# Make predictions on the evaluation dataset
prediction_prob_eval_model1 <- predict(logit_model_1, 
                                       crime_evaluation_data, type="response")

# Predict classes using the same threshold of 0.5
prediction_class_eval_model1 <- ifelse(logit_model_1 > 
                                         threshold, 1, 0)
print(prediction_class_eval_model1)
```
### Model 2 (Manually Selected Variables)
For Model 2, we manually select the predictor variables based on general considerations. We include ‘nox,’ which represents the nitrogen oxides concentration (in parts per 10 million), due to previous studies suggesting that high levels of air pollutants may correlate with increased crime rates. We also include ‘rad,’ the index of accessibility to radial highways, on the assumption that easier access to highways might facilitate the escape of individuals from crime scenes. The variable ‘tax,’ which represents the full-value property tax rate per $10,000, is included to examine whether neighborhoods with higher property tax rates might experience lower crime rates. Additionally, we include ‘ptratio,’ the pupil-teacher ratio by town, and ‘rm,’ the average number of rooms per dwelling, to explore potential relationships with crime rates, as studies suggest that individuals with lower-quality education may be more likely to engage in criminal activities later in life.

```{r, echo=FALSE}
library(DHARMa)

# Model 2, choosing the predictive variables manually
model2 <- glm(target ~ nox + rad + tax + ptratio + rm, 
              data = crime_training_data, family = "binomial")
summary(model2)

# plot residual vs. predicted plot
sr2 <- simulateResiduals(model2)
plot(sr2)

# get the log likelihood for comparison with other models
ll_model2 <- logLik(model2)
ll_model2
```

# **Select Models**
### Model 2 (Confusion Matrix, ROC curve, and AUC value)
```{r, echo=FALSE}
library(caret)
# Predict probabilities on the training data
prediction_prob_train <- predict(model2, crime_training_data, type="response")

# Predict classes (binary classification using a threshold of 0.5)
threshold <- 0.5
prediction_class_train <- ifelse(prediction_prob_train > threshold, 1, 0)

# Calculate performance metrics on the training dataset
# Confusion Matrix
conf_matrix_train <- confusionMatrix(as.factor(prediction_class_train), 
                                     as.factor(crime_training_data$target))
print(conf_matrix_train)

# Accuracy
accuracy <- sum(prediction_class_train == crime_training_data$target) / 
  length(crime_training_data$target)
cat("Accuracy: ", accuracy, "\n")

# Classification Error Rate
classification_error_rate <- 1 - accuracy
cat("Classification Error Rate: ", classification_error_rate, "\n")

# Precision
precision <- conf_matrix_train$byClass["Pos Pred Value"]
cat("Precision: ", precision, "\n")

# Sensitivity (Recall or True Positive Rate)
sensitivity <- conf_matrix_train$byClass["Sensitivity"]
cat("Sensitivity: ", sensitivity, "\n")

# Specificity (True Negative Rate)
specificity <- conf_matrix_train$byClass["Specificity"]
cat("Specificity: ", specificity, "\n")

# F1 Score
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("F1 Score: ", f1_score, "\n")

# AUC (Area Under the Curve)
roc_curve <- roc(crime_training_data$target, prediction_prob_train)
auc_value <- auc(roc_curve)
cat("AUC: ", auc_value, "\n")

# Plot ROC curve with AUC in the title
plot(roc_curve, main = paste("ROC Curve Model 2 - AUC:", 
                             round(auc(roc_curve), 2)), col = "red")

```
# **Predictions Using Evaluation Data Set**
### Model 2
```{r}
# Make predictions on the evaluation dataset
prediction_prob_eval_model2 <- predict(model2, 
                                       crime_evaluation_data, type="response")

# Predict classes using the same threshold of 0.5
prediction_class_eval_model2 <- ifelse(prediction_prob_eval_model2 > 
                                         threshold, 1, 0)
print(prediction_class_eval_model2)
```

### Model 3 (Manually Selected Other Variables)
For Model 3, we manually select predictor variables based on other relevant considerations. We include 'indus,' which indicates the proportion of non-retail business acres per suburb. A higher proportion may suggest increased urban density, potentially linked to crime rates. We also use 'nox,' measuring nitrogen oxides concentration (in parts per 10 million), since areas with higher pollution levels often have lower-income populations due to industrial activity, heavy traffic, or poor urban planning. The variable 'rm,' representing the average number of rooms per dwelling, may indicate affluence and its relationship to crime. Additionally, 'rad' assesses highway accessibility, which may facilitate crime due to ease of movement. Finally, we include 'lstat,' a measure of socioeconomic status that is often associated with crime trends.

```{r, echo=FALSE}
# Model 3, choosing new predictive variables manually
model3 <- glm(target ~ indus + nox + rm + rad + lstat, data = crime_training_data, family = "binomial")
summary(model3)

# plot residual vs. predicted plot
sr3 <- simulateResiduals(model3)
plot(sr3)

# get the log likelihood for comparison with other models
ll_model3 <- logLik(model3)
ll_model3
```

# **Select Models**
### Model 3 (Confusion Matrix, ROC curve, and AUC value)
```{r, echo=FALSE}
# Predict probabilities on the training data
prediction_prob_train3 <- predict(model3, crime_training_data, type="response")

# Predict classes (binary classification using a threshold of 0.5)
prediction_class_train3 <- ifelse(prediction_prob_train3 > threshold, 1, 0)

# Calculate performance metrics on the training dataset
# Confusion Matrix
conf_matrix_train3 <- confusionMatrix(as.factor(prediction_class_train3), 
                                     as.factor(crime_training_data$target))
print(conf_matrix_train3)

# Accuracy
accuracy3 <- sum(prediction_class_train3 == crime_training_data$target) / 
  length(crime_training_data$target)
cat("Accuracy: ", accuracy3, "\n")

# Classification Error Rate
classification_error_rate3 <- 1 - accuracy3
cat("Classification Error Rate: ", classification_error_rate3, "\n")

# Precision
precision3 <- conf_matrix_train3$byClass["Pos Pred Value"]
cat("Precision: ", precision3, "\n")

# Sensitivity (Recall or True Positive Rate)
sensitivity3 <- conf_matrix_train3$byClass["Sensitivity"]
cat("Sensitivity: ", sensitivity3, "\n")

# Specificity (True Negative Rate)
specificity3 <- conf_matrix_train3$byClass["Specificity"]
cat("Specificity: ", specificity3, "\n")

# F1 Score
f1_score3 <- 2 * (precision3 * sensitivity3) / (precision3 + sensitivity3)
cat("F1 Score: ", f1_score3, "\n")

# AUC (Area Under the Curve)
roc_curve3 <- roc(crime_training_data$target, prediction_prob_train3)
auc_value3 <- auc(roc_curve3)
cat("AUC: ", auc_value3, "\n")

# Plot ROC curve with AUC in the title
plot(roc_curve3, main = paste("ROC Curve Model 2 - AUC:", 
                             round(auc(roc_curve3), 2)), col = "green4")

```

# **Predictions Using Evaluation Data Set**
### Model 3
```{r}
# Make predictions on the evaluation dataset
prediction_prob_eval_model3 <- predict(model3, 
                                       crime_evaluation_data, type="response")

# Predict classes using the same threshold of 0.5
prediction_class_eval_model3 <- ifelse(prediction_prob_eval_model3 > 
                                         threshold, 1, 0)
print(prediction_class_eval_model3)
```
