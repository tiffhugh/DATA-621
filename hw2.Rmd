---
title: "hw2"
author: "Bedros, Haig; Dela Cruz, Sandra; Hugh, Tiffany; Li, Yanyi"
date: "2025-03-02"
output: html_document
        pdf_document
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(pROC)
```

### DATA EXPLORATION

```{r}
# Import data
data <- read.csv("https://raw.githubusercontent.com/tiffhugh/DATA-621/refs/heads/main/classification-output-data.csv")
glimpse(data)
```

```{r}
# Creating Confusion Matrix Using table() Function
conf_matrix <- table(Predicted = data$scored.class, Actual = data$class)
print(conf_matrix)
```

**2. Do the rows represent the actual or predicted class? The columns?**

In the confusion matrix, the columns labeled "Actual" represent the true labels from the dataset, while the rows labeled "Predicted" indicate the model's predictions. This means that the model correctly predicted 119 true negatives, 27 true positives, made 5 false positive predictions, and had 30 false negative predictions.

### Manually Creating Functions

```{r}
# Extracting values from the confusion matrix
  TP <- conf_matrix[2,2]  # True Positives
  TN <- conf_matrix[1,1]  # True Negatives
  FP <- conf_matrix[2,1]  # False Positives
  FN <- conf_matrix[1,2]  # False Negatives
```

**3. The function for Accuracy of the model:**

```{r}
accuracy = (TP+TN) / (TP+FP+TN+FN)
print(accuracy)
```
80.66% accuracy means the model is correct about 80.7% of the time.

**The function for Classification Error Rate of the model:**

```{r}
class_error_rate = (FP+FN) / (TP+FP+TN+FN)
print(class_error_rate)
```

19.34% classification error rate means the model misclassifies about 19.34% of the cases.

**The function for Precision of the model:**

```{r}
precision = (TP) / (TP+FP)
print(precision)
```
84.38% precision means that when the model predicts positive, it is correct 84.38% of the time.

**The function for Sensitivity of the model:**

```{r}
sensitivity = (TP) / (TP+FN)
print(sensitivity)
```
47.37% sensitivity means the model is identifying about 47.37% of actual positives.

**The function for Specificity of the model:**

```{r}
specificity = (TN) / (TN+FP)
print(specificity)
```

96% specificity means the model is very good at correctly identifying negatives.

**The function for F1 score of the model:**

```{r}
f1_score = (2*precision*sensitivity) / (precision+sensitivity)
print(f1_score)
```
60.7% means the model has a moderate balance between precision and sensitivity.


